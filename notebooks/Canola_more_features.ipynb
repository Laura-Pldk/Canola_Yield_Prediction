{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9276b0",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb58599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sktime.transformations.series.detrend import Detrender\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from standard_precip.spi import SPI\n",
    "from standard_precip.utils import plot_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06824890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Canola_detrend</th>\n",
       "      <th>spi_4</th>\n",
       "      <th>spi_5</th>\n",
       "      <th>spi_6</th>\n",
       "      <th>spi_7</th>\n",
       "      <th>spi_8</th>\n",
       "      <th>spi_9</th>\n",
       "      <th>spi_10</th>\n",
       "      <th>avg_max_temp_4</th>\n",
       "      <th>avg_max_temp_5</th>\n",
       "      <th>avg_max_temp_6</th>\n",
       "      <th>avg_max_temp_7</th>\n",
       "      <th>avg_max_temp_8</th>\n",
       "      <th>avg_max_temp_9</th>\n",
       "      <th>avg_max_temp_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>0.835685</td>\n",
       "      <td>1.552420</td>\n",
       "      <td>0.398286</td>\n",
       "      <td>1.608689</td>\n",
       "      <td>-1.138548</td>\n",
       "      <td>-0.619148</td>\n",
       "      <td>0.699883</td>\n",
       "      <td>1.150625</td>\n",
       "      <td>284.674011</td>\n",
       "      <td>291.586487</td>\n",
       "      <td>295.451691</td>\n",
       "      <td>297.247833</td>\n",
       "      <td>301.780426</td>\n",
       "      <td>292.659363</td>\n",
       "      <td>285.051239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>1.019651</td>\n",
       "      <td>0.310719</td>\n",
       "      <td>0.272510</td>\n",
       "      <td>-0.248256</td>\n",
       "      <td>-0.270669</td>\n",
       "      <td>-0.278103</td>\n",
       "      <td>-0.179716</td>\n",
       "      <td>-0.901951</td>\n",
       "      <td>282.146759</td>\n",
       "      <td>292.675629</td>\n",
       "      <td>296.554260</td>\n",
       "      <td>296.034912</td>\n",
       "      <td>299.861603</td>\n",
       "      <td>291.575226</td>\n",
       "      <td>282.966553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>3.180447</td>\n",
       "      <td>0.277076</td>\n",
       "      <td>-0.334090</td>\n",
       "      <td>0.603571</td>\n",
       "      <td>0.403538</td>\n",
       "      <td>0.782308</td>\n",
       "      <td>2.709394</td>\n",
       "      <td>0.711722</td>\n",
       "      <td>283.948242</td>\n",
       "      <td>291.120361</td>\n",
       "      <td>296.939117</td>\n",
       "      <td>298.817383</td>\n",
       "      <td>301.038696</td>\n",
       "      <td>291.306488</td>\n",
       "      <td>287.401154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>-0.681926</td>\n",
       "      <td>0.740812</td>\n",
       "      <td>1.373921</td>\n",
       "      <td>-1.551774</td>\n",
       "      <td>-0.435588</td>\n",
       "      <td>0.232750</td>\n",
       "      <td>-1.372053</td>\n",
       "      <td>-1.695853</td>\n",
       "      <td>281.231293</td>\n",
       "      <td>287.215179</td>\n",
       "      <td>296.320404</td>\n",
       "      <td>301.303864</td>\n",
       "      <td>296.313019</td>\n",
       "      <td>291.511688</td>\n",
       "      <td>288.256470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>-1.567468</td>\n",
       "      <td>2.002202</td>\n",
       "      <td>-0.518458</td>\n",
       "      <td>1.556986</td>\n",
       "      <td>0.509661</td>\n",
       "      <td>1.773032</td>\n",
       "      <td>0.895744</td>\n",
       "      <td>1.033422</td>\n",
       "      <td>275.753235</td>\n",
       "      <td>289.641205</td>\n",
       "      <td>294.908081</td>\n",
       "      <td>300.886871</td>\n",
       "      <td>296.023590</td>\n",
       "      <td>291.019562</td>\n",
       "      <td>285.494232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>0.523820</td>\n",
       "      <td>0.532937</td>\n",
       "      <td>-1.697363</td>\n",
       "      <td>0.961862</td>\n",
       "      <td>-1.446509</td>\n",
       "      <td>0.499695</td>\n",
       "      <td>-1.474134</td>\n",
       "      <td>-2.029217</td>\n",
       "      <td>283.979858</td>\n",
       "      <td>292.507629</td>\n",
       "      <td>296.083862</td>\n",
       "      <td>299.666107</td>\n",
       "      <td>300.495178</td>\n",
       "      <td>297.305878</td>\n",
       "      <td>283.504913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>0.591938</td>\n",
       "      <td>-1.059830</td>\n",
       "      <td>0.273647</td>\n",
       "      <td>-0.688714</td>\n",
       "      <td>-0.430323</td>\n",
       "      <td>-0.897585</td>\n",
       "      <td>1.339607</td>\n",
       "      <td>-1.142240</td>\n",
       "      <td>290.806793</td>\n",
       "      <td>297.221161</td>\n",
       "      <td>296.493164</td>\n",
       "      <td>300.909698</td>\n",
       "      <td>296.245300</td>\n",
       "      <td>290.973480</td>\n",
       "      <td>287.136871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>0.636888</td>\n",
       "      <td>-0.105057</td>\n",
       "      <td>0.681106</td>\n",
       "      <td>-1.141082</td>\n",
       "      <td>-0.218658</td>\n",
       "      <td>-0.842599</td>\n",
       "      <td>1.921957</td>\n",
       "      <td>-0.756340</td>\n",
       "      <td>283.612152</td>\n",
       "      <td>293.872833</td>\n",
       "      <td>297.419464</td>\n",
       "      <td>299.356598</td>\n",
       "      <td>300.507751</td>\n",
       "      <td>295.736267</td>\n",
       "      <td>287.023193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>-2.341333</td>\n",
       "      <td>0.840549</td>\n",
       "      <td>0.646930</td>\n",
       "      <td>-0.982525</td>\n",
       "      <td>0.189466</td>\n",
       "      <td>-1.968555</td>\n",
       "      <td>0.959279</td>\n",
       "      <td>-0.739282</td>\n",
       "      <td>276.046600</td>\n",
       "      <td>286.521606</td>\n",
       "      <td>296.374207</td>\n",
       "      <td>300.664185</td>\n",
       "      <td>297.919647</td>\n",
       "      <td>294.441315</td>\n",
       "      <td>284.963867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>-10.342722</td>\n",
       "      <td>-1.357788</td>\n",
       "      <td>-2.438554</td>\n",
       "      <td>-0.763792</td>\n",
       "      <td>-0.754718</td>\n",
       "      <td>2.122007</td>\n",
       "      <td>0.205853</td>\n",
       "      <td>0.806743</td>\n",
       "      <td>289.224030</td>\n",
       "      <td>296.703491</td>\n",
       "      <td>297.945221</td>\n",
       "      <td>301.438324</td>\n",
       "      <td>295.128296</td>\n",
       "      <td>292.048950</td>\n",
       "      <td>283.809509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Canola_detrend     spi_4     spi_5     spi_6     spi_7     spi_8  \\\n",
       "1971        0.835685  1.552420  0.398286  1.608689 -1.138548 -0.619148   \n",
       "1972        1.019651  0.310719  0.272510 -0.248256 -0.270669 -0.278103   \n",
       "1973        3.180447  0.277076 -0.334090  0.603571  0.403538  0.782308   \n",
       "1974       -0.681926  0.740812  1.373921 -1.551774 -0.435588  0.232750   \n",
       "1975       -1.567468  2.002202 -0.518458  1.556986  0.509661  1.773032   \n",
       "1976        0.523820  0.532937 -1.697363  0.961862 -1.446509  0.499695   \n",
       "1977        0.591938 -1.059830  0.273647 -0.688714 -0.430323 -0.897585   \n",
       "1978        0.636888 -0.105057  0.681106 -1.141082 -0.218658 -0.842599   \n",
       "1979       -2.341333  0.840549  0.646930 -0.982525  0.189466 -1.968555   \n",
       "1980      -10.342722 -1.357788 -2.438554 -0.763792 -0.754718  2.122007   \n",
       "\n",
       "         spi_9    spi_10  avg_max_temp_4  avg_max_temp_5  avg_max_temp_6  \\\n",
       "1971  0.699883  1.150625      284.674011      291.586487      295.451691   \n",
       "1972 -0.179716 -0.901951      282.146759      292.675629      296.554260   \n",
       "1973  2.709394  0.711722      283.948242      291.120361      296.939117   \n",
       "1974 -1.372053 -1.695853      281.231293      287.215179      296.320404   \n",
       "1975  0.895744  1.033422      275.753235      289.641205      294.908081   \n",
       "1976 -1.474134 -2.029217      283.979858      292.507629      296.083862   \n",
       "1977  1.339607 -1.142240      290.806793      297.221161      296.493164   \n",
       "1978  1.921957 -0.756340      283.612152      293.872833      297.419464   \n",
       "1979  0.959279 -0.739282      276.046600      286.521606      296.374207   \n",
       "1980  0.205853  0.806743      289.224030      296.703491      297.945221   \n",
       "\n",
       "      avg_max_temp_7  avg_max_temp_8  avg_max_temp_9  avg_max_temp_10  \n",
       "1971      297.247833      301.780426      292.659363       285.051239  \n",
       "1972      296.034912      299.861603      291.575226       282.966553  \n",
       "1973      298.817383      301.038696      291.306488       287.401154  \n",
       "1974      301.303864      296.313019      291.511688       288.256470  \n",
       "1975      300.886871      296.023590      291.019562       285.494232  \n",
       "1976      299.666107      300.495178      297.305878       283.504913  \n",
       "1977      300.909698      296.245300      290.973480       287.136871  \n",
       "1978      299.356598      300.507751      295.736267       287.023193  \n",
       "1979      300.664185      297.919647      294.441315       284.963867  \n",
       "1980      301.438324      295.128296      292.048950       283.809509  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load df\n",
    "feature_df = pd.read_csv('../data/feature_df.csv', header=0)\n",
    "feature_df.index = range(1971, 2023)\n",
    "feature_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9950d7c7",
   "metadata": {},
   "source": [
    "## Weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87a49c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t2m</th>\n",
       "      <th>tp</th>\n",
       "      <th>Canola</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1971-04-01 00:00:00</th>\n",
       "      <td>267.04202</td>\n",
       "      <td>0.010006</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-04-01 01:00:00</th>\n",
       "      <td>266.61612</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-04-01 02:00:00</th>\n",
       "      <td>266.40143</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-04-01 03:00:00</th>\n",
       "      <td>266.30045</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-04-01 04:00:00</th>\n",
       "      <td>266.17166</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           t2m        tp  Canola\n",
       "time                                            \n",
       "1971-04-01 00:00:00  267.04202  0.010006    18.0\n",
       "1971-04-01 01:00:00  266.61612  0.000009    18.0\n",
       "1971-04-01 02:00:00  266.40143  0.000017    18.0\n",
       "1971-04-01 03:00:00  266.30045  0.000024    18.0\n",
       "1971-04-01 04:00:00  266.17166  0.000031    18.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load raw weather data with t2m, tp \n",
    "weather_raw = pd.read_csv('C:/Users/maris/python_notebooks/XAI_TS_Forecasting/notebooks/dist1_t2m_tp.csv', header=0)\n",
    "weather_raw['time'] = pd.to_datetime(weather_raw['time'])\n",
    "weather_raw = weather_raw.set_index('time')\n",
    "\n",
    "# check for nan\n",
    "contains_nan = weather_raw.isna().any().any()\n",
    "\n",
    "weather_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34c50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the longest consecutive true streak\n",
    "\n",
    "def longest_consecutive_true_streak(series):\n",
    "    \n",
    "    # Convert the series to integers (True to 1, False to 0) for easier streak calculation\n",
    "    as_ints = series.astype(int)\n",
    "    # Calculate the difference to identify changes in streaks\n",
    "    diff = as_ints.diff()\n",
    "    # Start a new group every time there's a change from 0 to 1 (start of a new streak)\n",
    "    groups = (diff == 1).cumsum()\n",
    "    # Use the groups to isolate consecutive trues, then count them, keeping the max\n",
    "    streak_lengths = as_ints.groupby(groups).sum()\n",
    "    # Return the length of the longest streak\n",
    "    return streak_lengths.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017c074",
   "metadata": {},
   "source": [
    "## Heat Wave Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a4420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max temp for one day \n",
    "dist1_df_hot = weather_raw \n",
    "\n",
    "# add extra columns containing the years and the months\n",
    "dist1_df_hot['year'] = dist1_df_hot.index.year\n",
    "dist1_df_hot['month'] = dist1_df_hot.index.month\n",
    "dist1_df_hot['day'] = dist1_df_hot.index.day\n",
    "\n",
    "# get variables year, month and canola from old data frame \n",
    "daily_df = weather_raw.resample('D').first()\n",
    "daily_df = daily_df[[\"Canola\",\"year\",\"month\"]]\n",
    "\n",
    "# Resample the data to daily frequency and get the maximum temperature for each day\n",
    "daily_max_temperature = dist1_df_hot['t2m'].resample('D').max()\n",
    "\n",
    "#drop all nan ( values for month november-march)\n",
    "daily_max_temperature = daily_max_temperature.dropna()\n",
    "\n",
    "# divide in test and training data \n",
    "training_data = daily_max_temperature.loc[:'1989']\n",
    "testing_data = daily_max_temperature.loc['1990':]\n",
    "\n",
    "# calculate for every day the 90% quantile\n",
    "quantile_90_series = training_data.groupby([training_data.index.month, training_data.index.day]).quantile(0.9)\n",
    "quantile_90_series.index = quantile_90_series.index.map(lambda x: f\"{x[0]:02d}-{x[1]:02d}\")\n",
    "\n",
    "\n",
    "test_df = testing_data.to_frame(name='value')\n",
    "test_df['month_day'] = test_df.index.strftime('%m-%d')\n",
    "\n",
    "# map the 90th percentile values from quantile_90_series to the test series\n",
    "test_df['quantile_90'] = test_df['month_day'].apply(lambda x: quantile_90_series.get(x, pd.NA))\n",
    "\n",
    "# compare each test value to its corresponding 90th percentile value\n",
    "test_df['is_above_quantile_90'] = test_df['value'] > test_df['quantile_90']\n",
    "test_df.drop(['month_day', 'quantile_90'], axis=1, inplace=True)\n",
    "\n",
    "# Group the DataFrame by year, and apply the function to find the longest streak of True values\n",
    "longest_streak_by_year = test_df.groupby(test_df.index.year)['is_above_quantile_90'].apply(longest_consecutive_true_streak)\n",
    "\n",
    "#print(longest_streak_by_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30505242",
   "metadata": {},
   "source": [
    "## Summer days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94dd1d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "1990    79\n",
       "1991    52\n",
       "1992    40\n",
       "1993    29\n",
       "1994    40\n",
       "1995    56\n",
       "1996    57\n",
       "1997    69\n",
       "1998    55\n",
       "1999    38\n",
       "2000    40\n",
       "2001    59\n",
       "2002    59\n",
       "2003    67\n",
       "2004    28\n",
       "2005    47\n",
       "2006    72\n",
       "2007    67\n",
       "2008    48\n",
       "2009    56\n",
       "2010    46\n",
       "2011    62\n",
       "2012    73\n",
       "2013    50\n",
       "2014    47\n",
       "2015    64\n",
       "2016    55\n",
       "2017    68\n",
       "2018    73\n",
       "2019    60\n",
       "2020    81\n",
       "2021    94\n",
       "2022    71\n",
       "Name: above_25, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = daily_max_temperature.loc[:'1989']\n",
    "testing_data = daily_max_temperature.loc['1990':]\n",
    "\n",
    "test_df_summer = testing_data.to_frame(name='t2m')\n",
    "test_df_summer['month_day'] = test_df_summer.index.strftime('%m-%d')\n",
    "\n",
    "# # map the 90th percentile values from quantile_90_series to the test series\n",
    "# test_df['quantile_90'] = test_df['month_day'].apply(lambda x: quantile_90_series.get(x, pd.NA))\n",
    "\n",
    "# compare each test value to its corresponding 90th percentile value\n",
    "test_df_summer['above_25'] = test_df_summer['t2m'] > 298\n",
    "\n",
    "test_df_summer[\"above_25\"].iloc[100:120]\n",
    "\n",
    "days_over_25 = test_df_summer.groupby(test_df_summer.index.year)['above_25'].apply(sum)\n",
    "\n",
    "days_over_25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ad887b",
   "metadata": {},
   "source": [
    "## Cold Wave Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79c475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from above with adjustments to the coldness\n",
    "\n",
    "# get max temp for one day \n",
    "dist1_df_hot = weather_raw \n",
    "\n",
    "# add extra columns containing the years and the months\n",
    "dist1_df_hot['year'] = dist1_df_hot.index.year\n",
    "dist1_df_hot['month'] = dist1_df_hot.index.month\n",
    "dist1_df_hot['day'] = dist1_df_hot.index.day\n",
    "\n",
    "# get variables year, month and canola from old data frame \n",
    "daily_df = weather_raw.resample('D').first()\n",
    "daily_df = daily_df[[\"Canola\",\"year\",\"month\"]]\n",
    "\n",
    "# Resample the data to daily frequency and get the maximum temperature for each day\n",
    "daily_min_temperature = dist1_df_hot['t2m'].resample('D').min()\n",
    "\n",
    "#drop all nan ( values for month november-march)\n",
    "daily_min_temperature = daily_min_temperature.dropna()\n",
    "\n",
    "# divide in test and training data \n",
    "training_data = daily_min_temperature.loc[:'1989']\n",
    "testing_data = daily_min_temperature.loc['1990':]\n",
    "\n",
    "# calculate for every day the 90% quantile\n",
    "quantile_10_series = training_data.groupby([training_data.index.month, training_data.index.day]).quantile(0.1)\n",
    "quantile_10_series.index = quantile_10_series.index.map(lambda x: f\"{x[0]:02d}-{x[1]:02d}\")\n",
    "\n",
    "\n",
    "test_df_cold = testing_data.to_frame(name='value')\n",
    "test_df_cold['month_day'] = test_df_cold.index.strftime('%m-%d')\n",
    "\n",
    "# map the 90th percentile values from quantile_90_series to the test series\n",
    "test_df_cold['quantile_10'] = test_df_cold['month_day'].apply(lambda x: quantile_10_series.get(x, pd.NA))\n",
    "\n",
    "# compare each test value to its corresponding 90th percentile value\n",
    "test_df_cold['is_under_quantile_10'] = test_df_cold['value'] < test_df_cold['quantile_10']\n",
    "test_df_cold.drop(['month_day', 'quantile_10'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Group the DataFrame by year, and apply the function to find the longest streak of True values\n",
    "longest_streak_by_year_cold = test_df_cold.groupby(test_df.index.year)['is_under_quantile_10'].apply(longest_consecutive_true_streak)\n",
    "\n",
    "#print(longest_streak_by_year_cold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa6e1d",
   "metadata": {},
   "source": [
    "## Frost Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c7148d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "1990     0\n",
       "1991     4\n",
       "1992     3\n",
       "1993     1\n",
       "1994     0\n",
       "1995     2\n",
       "1996     8\n",
       "1997     7\n",
       "1998     0\n",
       "1999     1\n",
       "2000     3\n",
       "2001     2\n",
       "2002     8\n",
       "2003     6\n",
       "2004     1\n",
       "2005     0\n",
       "2006     1\n",
       "2007     4\n",
       "2008     0\n",
       "2009     4\n",
       "2010     1\n",
       "2011     0\n",
       "2012     2\n",
       "2013    11\n",
       "2014     6\n",
       "2015     0\n",
       "2016     0\n",
       "2017     0\n",
       "2018     9\n",
       "2019     3\n",
       "2020    11\n",
       "2021     1\n",
       "2022     6\n",
       "Name: under_0, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = daily_max_temperature.loc[:'1989']\n",
    "testing_data = daily_max_temperature.loc['1990':]\n",
    "\n",
    "test_df_frost = testing_data.to_frame(name='t2m')\n",
    "test_df_frost['month_day'] = test_df_frost.index.strftime('%m-%d')\n",
    "\n",
    "# compare each test value to its corresponding 90th percentile value\n",
    "test_df_frost['under_0'] = test_df_frost['t2m'] < 273\n",
    "\n",
    "#test_df_frost[\"under_0\"].iloc[100:120]\n",
    "\n",
    "days_under_0 = test_df_frost.groupby(test_df_frost.index.year)['under_0'].apply(sum)\n",
    "\n",
    "days_under_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2651020",
   "metadata": {},
   "source": [
    "## Longest Dry Spell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8999047b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maris\\AppData\\Local\\Temp\\ipykernel_52200\\3952656548.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dist1_df_perci_wo0['less_than_0.001'] = dist1_df_perci_wo0['tp'] < 0.001\n"
     ]
    }
   ],
   "source": [
    "dist1_df_perci = weather_raw.resample('D').sum()\n",
    "\n",
    "# use data frame without zero to calculate the 5% quantile (only!)\n",
    "dist1_df_perci_wo0 = dist1_df_perci[dist1_df_perci['tp'] != 0]\n",
    "\n",
    "# Threshhold is 1mm or 0.001m, here precipitation is likely measured in m \n",
    "\n",
    "# find days with less than 0.001m rain \n",
    "dist1_df_perci_wo0['less_than_0.001'] = dist1_df_perci_wo0['tp'] < 0.001\n",
    "\n",
    "# apply function \n",
    "longest_dry_spell_per_year = dist1_df_perci_wo0.groupby(dist1_df_perci_wo0.index.year)['less_than_0.001'].apply(longest_consecutive_true_streak)\n",
    "\n",
    "#longest_dry_spell_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e8591",
   "metadata": {},
   "source": [
    "## Longest Wet Spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "920edbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maris\\AppData\\Local\\Temp\\ipykernel_52200\\3498330211.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dist1_df_perci_wo0['more_than_0.001'] = dist1_df_perci_wo0['tp'] > 0.001\n"
     ]
    }
   ],
   "source": [
    "dist1_df_perci = weather_raw.resample('D').sum()\n",
    "\n",
    "# use data frame without zero to calculate the 5% quantile (only!)\n",
    "dist1_df_perci_wo0 = dist1_df_perci[dist1_df_perci['tp'] != 0]\n",
    "\n",
    "# Threshhold is 1mm or 0.001m, here precipitation is likely measured in m \n",
    "\n",
    "# find days with less than 0.001m rain \n",
    "dist1_df_perci_wo0['more_than_0.001'] = dist1_df_perci_wo0['tp'] > 0.001\n",
    "\n",
    "# apply function \n",
    "longest_wet_spell_per_year = dist1_df_perci_wo0.groupby(dist1_df_perci_wo0.index.year)['more_than_0.001'].apply(longest_consecutive_true_streak)\n",
    "\n",
    "#longest_wet_spell_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba11800",
   "metadata": {},
   "source": [
    "## 95th Quantile Precipitation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82936104",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_pre = dist1_df_perci_wo0[\"tp\"].loc[:'1989']\n",
    "testing_data_pre = dist1_df_perci_wo0[\"tp\"].loc['1990':]\n",
    "\n",
    "# calculate for every day the 90% quantile\n",
    "quantile_95_series = training_data_pre.groupby([training_data_pre.index.month, training_data_pre.index.day]).quantile(0.95)\n",
    "quantile_95_series.index = quantile_95_series.index.map(lambda x: f\"{x[0]:02d}-{x[1]:02d}\")\n",
    "\n",
    "test_df_pre = testing_data_pre.to_frame(name='tp')\n",
    "test_df_pre['month_day'] = test_df_pre.index.strftime('%m-%d')\n",
    "\n",
    "# map the 90th percentile values from quantile_90_series to the test series\n",
    "test_df_pre['quantile_95'] = test_df_pre['month_day'].apply(lambda x: quantile_95_series.get(x, pd.NA))\n",
    "\n",
    "# compare each test value to its corresponding 90th percentile value\n",
    "test_df_pre['over_quantile_95'] = test_df_pre['tp'] > test_df_pre['quantile_95']\n",
    "test_df_pre.drop(['month_day', 'quantile_95'], axis=1, inplace=True)\n",
    "\n",
    "# Group the DataFrame by year, and apply the function to find the longest streak of True values\n",
    "days_over_95 = test_df_pre.groupby(test_df_pre.index.year)['over_quantile_95'].apply(sum)\n",
    "\n",
    "# days_over_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average max/min temp for whole growing season \n",
    "# average min temp per month "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

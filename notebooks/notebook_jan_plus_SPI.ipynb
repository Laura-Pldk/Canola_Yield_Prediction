{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:56:06.046909Z",
     "iopub.status.busy": "2024-03-02T14:56:06.046406Z",
     "iopub.status.idle": "2024-03-02T14:56:37.995486Z",
     "shell.execute_reply": "2024-03-02T14:56:37.993913Z",
     "shell.execute_reply.started": "2024-03-02T14:56:06.046873Z"
    }
   },
   "outputs": [],
   "source": [
    "#! pip install sktime\n",
    "#! pip install standard-precip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import xarray as xr\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sktime.transformations.series.detrend import Detrender\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "from scipy.stats import gamma\n",
    "\n",
    "from standard_precip.spi import SPI\n",
    "from standard_precip.utils import plot_index\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:21:42.951083Z",
     "iopub.status.busy": "2024-03-02T14:21:42.949923Z",
     "iopub.status.idle": "2024-03-02T14:21:43.066636Z",
     "shell.execute_reply": "2024-03-02T14:21:43.064980Z",
     "shell.execute_reply.started": "2024-03-02T14:21:42.951037Z"
    }
   },
   "outputs": [],
   "source": [
    "#read dataframe \n",
    "# canola_2 = df = pd.read_csv('/kaggle/input/rm-yields-data/rm-yields-data.csv', header=0, index_col=0, parse_dates=True)\n",
    "# canola_small = canola_2.iloc[:, [0, 2]].copy()\n",
    "\n",
    "#read dataframe \n",
    "canola_2 = df = pd.read_csv('../data/rm-yields-data.csv', header=0, index_col=0, parse_dates=True)\n",
    "canola_small = canola_2.iloc[:, [0, 2]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:21:43.069636Z",
     "iopub.status.busy": "2024-03-02T14:21:43.069156Z",
     "iopub.status.idle": "2024-03-02T14:21:43.194146Z",
     "shell.execute_reply": "2024-03-02T14:21:43.192899Z",
     "shell.execute_reply.started": "2024-03-02T14:21:43.069600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "start_year = 1938\n",
    "start_analysis = 1990\n",
    "exclude_years = start_analysis - start_year\n",
    "#cut 70s and 80s as well \n",
    "#cut of first 52 observations (NAs)\n",
    "canola_small.drop(canola_small.index[:exclude_years], inplace=True)\n",
    "\n",
    "#filter out every observation that contains NAs\n",
    "canola_filtered = canola_small.groupby('RM').filter(lambda group: not group['Canola'].isnull().any())\n",
    "\n",
    "# how may districts? 148\n",
    "num_districts = canola_filtered.groupby('RM').ngroups\n",
    "print(num_districts)\n",
    "#excluding 70s and 80s lead to 36 more colmplete districts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:21:43.197420Z",
     "iopub.status.busy": "2024-03-02T14:21:43.197030Z",
     "iopub.status.idle": "2024-03-02T14:21:43.230428Z",
     "shell.execute_reply": "2024-03-02T14:21:43.229165Z",
     "shell.execute_reply.started": "2024-03-02T14:21:43.197388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group by 'RM' and check if 'Canola' has any missing values in each group\n",
    "districts_with_full_data = canola_filtered.groupby('RM')['Canola'].apply(lambda group: not group.isnull().any())\n",
    "\n",
    "# Extract the list of districts with full data\n",
    "districts_with_full_data_list = districts_with_full_data[districts_with_full_data].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:21:43.232773Z",
     "iopub.status.busy": "2024-03-02T14:21:43.232301Z",
     "iopub.status.idle": "2024-03-02T14:21:44.783351Z",
     "shell.execute_reply": "2024-03-02T14:21:44.781418Z",
     "shell.execute_reply.started": "2024-03-02T14:21:43.232733Z"
    }
   },
   "outputs": [],
   "source": [
    "# select weather data\n",
    "#open only the years from 1990 til 2022\n",
    "\n",
    "# Define the directory path and pattern for the NetCDF files\n",
    "# directory_path = '/kaggle/input/copernicus-data/'\n",
    "# file_pattern = '*.nc'\n",
    "directory_path = '../data/all_raw_data/'\n",
    "file_pattern = 'data_*.nc'\n",
    "\n",
    "\n",
    "# Get a list of files matching the pattern\n",
    "files_to_open = glob.glob(os.path.join(directory_path, file_pattern))\n",
    "\n",
    "# Open only the files for the years 1990 to 2022\n",
    "years_to_open = list(map(str, range(start_analysis, 2023)))\n",
    "files_to_open = [file for file in files_to_open if any(year in file for year in years_to_open)]\n",
    "\n",
    "# Use open_mfdataset to open the selected files\n",
    "cop_all_90 = xr.open_mfdataset(files_to_open, combine='by_coords')\n",
    "\n",
    "\n",
    "# open data from 1971 til 1989 as training data \n",
    "\n",
    "training_years_to_open = list(map(str, range(1971, 1989)))\n",
    "training_files_to_open = [file for file in files_to_open if any(year in file for year in years_to_open)]\n",
    "\n",
    "# Use open_mfdataset to open the selected files\n",
    "training_data = xr.open_mfdataset(training_files_to_open, combine='by_coords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:21:44.785543Z",
     "iopub.status.busy": "2024-03-02T14:21:44.785094Z",
     "iopub.status.idle": "2024-03-02T14:21:44.800053Z",
     "shell.execute_reply": "2024-03-02T14:21:44.798241Z",
     "shell.execute_reply.started": "2024-03-02T14:21:44.785507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (longitude: 88, latitude: 41, time: 169488)\n",
      "Coordinates:\n",
      "  * longitude  (longitude) float32 -110.0 -109.9 -109.8 ... -101.5 -101.4 -101.3\n",
      "  * latitude   (latitude) float32 53.0 52.9 52.8 52.7 ... 49.3 49.2 49.1 49.0\n",
      "  * time       (time) datetime64[ns] 1990-04-01 ... 2022-10-31T23:00:00\n",
      "Data variables:\n",
      "    t2m        (time, latitude, longitude) float32 dask.array<chunksize=(5136, 41, 88), meta=np.ndarray>\n",
      "    tp         (time, latitude, longitude) float32 dask.array<chunksize=(5136, 41, 88), meta=np.ndarray>\n",
      "Attributes:\n",
      "    Conventions:  CF-1.6\n",
      "    history:      2024-01-23 18:32:47 GMT by grib_to_netcdf-2.24.0: /opt/ecmw...\n"
     ]
    }
   ],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:21:44.803398Z",
     "iopub.status.busy": "2024-03-02T14:21:44.802833Z",
     "iopub.status.idle": "2024-03-02T14:21:44.944562Z",
     "shell.execute_reply": "2024-03-02T14:21:44.943192Z",
     "shell.execute_reply.started": "2024-03-02T14:21:44.803353Z"
    }
   },
   "outputs": [],
   "source": [
    "# # center points for regions\n",
    "# df_regions = pd.read_csv('/kaggle/input/cgn-sk-csv-eng/cgn_sk_csv_eng.csv')\n",
    "# df_rms = df_regions[['Geographical Name','Latitude', 'Longitude']][df_regions['Generic Term'] == 'Rural Municipality']\n",
    "# df_rms['region_index'] = df_rms['Geographical Name'].str.split(' ').str[-1].astype(int)\n",
    "\n",
    "# center points for regions\n",
    "df_regions = pd.read_csv(r'../data/cgn_sk_csv_eng.csv')\n",
    "df_rms = df_regions[['Geographical Name','Latitude', 'Longitude']][df_regions['Generic Term'] == 'Rural Municipality']\n",
    "df_rms['region_index'] = df_rms['Geographical Name'].str.split(' ').str[-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:21:44.946767Z",
     "iopub.status.busy": "2024-03-02T14:21:44.946237Z",
     "iopub.status.idle": "2024-03-02T14:21:44.965930Z",
     "shell.execute_reply": "2024-03-02T14:21:44.963742Z",
     "shell.execute_reply.started": "2024-03-02T14:21:44.946729Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(df_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:21:44.969938Z",
     "iopub.status.busy": "2024-03-02T14:21:44.969472Z",
     "iopub.status.idle": "2024-03-02T14:21:44.981012Z",
     "shell.execute_reply": "2024-03-02T14:21:44.979989Z",
     "shell.execute_reply.started": "2024-03-02T14:21:44.969901Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_center(region):\n",
    "    avg_lat = df_rms['Latitude'][df_rms['region_index'] == region].item()\n",
    "    avg_long = df_rms['Longitude'][df_rms['region_index'] == region].item()\n",
    "    return avg_lat, avg_long\n",
    "\n",
    "def detrend_ts(df_region):\n",
    "    # linear detrending\n",
    "    forecaster = PolynomialTrendForecaster(degree=2)\n",
    "    transformer = Detrender(forecaster=forecaster)\n",
    "    yt = transformer.fit_transform(df_region['Canola'])\n",
    "    return yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:21:44.986217Z",
     "iopub.status.busy": "2024-03-02T14:21:44.985710Z",
     "iopub.status.idle": "2024-03-02T14:21:44.998001Z",
     "shell.execute_reply": "2024-03-02T14:21:44.996380Z",
     "shell.execute_reply.started": "2024-03-02T14:21:44.986181Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_canola_weather_data(region = 310):\n",
    "    # select data from region with center point\n",
    "    center_lat, center_long = get_center(region)\n",
    "    cropped_data_tmp = cop_all_90.sel(longitude=center_long, latitude=center_lat,method='nearest')\n",
    "    cropped_data_tmp_train = training_data.sel(longitude=center_long, latitude=center_lat,method='nearest')\n",
    "    \n",
    "    # get residuals for canola yield\n",
    "    df_tmp = canola_filtered[canola_filtered['RM'] == region]\n",
    "    residuals = detrend_ts(df_tmp)\n",
    "\n",
    "    # merge weather data and canola residuals\n",
    "    df_weather_region = cropped_data_tmp.to_dataframe()\n",
    "    df_weather_region_train = cropped_data_tmp_train.to_dataframe()\n",
    "\n",
    "    df_weather_region['region'] = region\n",
    "\n",
    "    column_to_append = residuals.tolist()\n",
    "    years = df_weather_region.index.year\n",
    "    df_weather_region['Canola_detrended'] = [column_to_append[year - start_analysis] for year in years]\n",
    "    df_weather_region.drop(['longitude','latitude'],axis=1,inplace=True)\n",
    "    \n",
    "    df_weather_region_train['region'] = region\n",
    "\n",
    "    column_to_append = residuals.tolist()\n",
    "    years = df_weather_region_train.index.year\n",
    "    df_weather_region_train['Canola_detrended'] = [column_to_append[year - start_analysis] for year in years]\n",
    "    df_weather_region_train.drop(['longitude','latitude'],axis=1,inplace=True)\n",
    "\n",
    "    return df_weather_region, pd.DataFrame(residuals), df_weather_region_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:53:34.976249Z",
     "iopub.status.busy": "2024-03-02T14:53:34.975753Z",
     "iopub.status.idle": "2024-03-02T14:53:34.986414Z",
     "shell.execute_reply": "2024-03-02T14:53:34.985037Z",
     "shell.execute_reply.started": "2024-03-02T14:53:34.976217Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_spi(prcp_data, scale=1):\n",
    "    # Step 1: Calculate L-moments\n",
    "    n = len(prcp_data)\n",
    "    prcp_data_sorted = np.sort(prcp_data)\n",
    "    \n",
    "    # L-moment ratio\n",
    "    l_moment_1 = np.sum(prcp_data_sorted) / n\n",
    "    l_moment_2 = np.sum((2 * np.arange(1, n + 1) - 1 - n) * prcp_data_sorted) / (n ** 2)\n",
    "    \n",
    "    # Step 2: Estimate parameters of gamma distribution\n",
    "    k = l_moment_1 / l_moment_2\n",
    "    theta = l_moment_2 / k\n",
    "    \n",
    "    # Step 3: Calculate SPI values\n",
    "    spi_values = gamma.ppf((np.arange(1, n + 1) - 0.35) / (n + 0.3), a=k, scale=theta * scale)\n",
    "    \n",
    "    return spi_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:21:45.000496Z",
     "iopub.status.busy": "2024-03-02T14:21:45.000002Z",
     "iopub.status.idle": "2024-03-02T14:21:45.015856Z",
     "shell.execute_reply": "2024-03-02T14:21:45.014224Z",
     "shell.execute_reply.started": "2024-03-02T14:21:45.000459Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_temp_features(df_weather_region, df_year):\n",
    "    for month in range(4,11):    \n",
    "        daily_max_temperatures = df_weather_region.resample('D').max()\n",
    "        monthly_avg_max_temperatures = daily_max_temperatures.resample('MS').mean()\n",
    "        \n",
    "    #     dist1_df_month = dist1_df.resample('MS').mean()\n",
    "        month_data = monthly_avg_max_temperatures[monthly_avg_max_temperatures.index.month == month]\n",
    "        column_to_append = month_data['t2m'].tolist()\n",
    "        df_year.loc[:, f'average_max_temp_in_{month}'] = column_to_append\n",
    "    return df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:06:20.041980Z",
     "iopub.status.busy": "2024-03-02T15:06:20.041525Z",
     "iopub.status.idle": "2024-03-02T15:06:20.051218Z",
     "shell.execute_reply": "2024-03-02T15:06:20.049767Z",
     "shell.execute_reply.started": "2024-03-02T15:06:20.041946Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_spi_features(df_weather_region, df_year):\n",
    "    for month in range(4, 11):\n",
    "        # tried resampling in various ways but none worked\n",
    "        tp_in_month = df_weather_region[df_weather_region.index.month == month]\n",
    "\n",
    "        spi = SPI()\n",
    "\n",
    "        # Assuming spi.calculate is the SPI calculation function\n",
    "        spi_values = spi.calculate(\n",
    "            tp_in_month.reset_index(),\n",
    "            'time',\n",
    "            'tp',\n",
    "            freq=\"M\",\n",
    "            scale=1,\n",
    "            fit_type=\"lmom\",\n",
    "            dist_type=\"gam\"\n",
    "        )\n",
    "        \n",
    "        # may have to aggregate here somehow\n",
    "\n",
    "        # Add each SPI column separately\n",
    "        for col_name in spi_values.columns:\n",
    "            df_year[f'SPI_in_{month}_{col_name}'] = spi_values[col_name]\n",
    "            \n",
    "    return df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_summer_days(df_weather_region, df_year):\n",
    "\n",
    "    testing_data = df_weather_region['t2m'].resample('D').max()\n",
    "    testing_data = testing_data.dropna()\n",
    "    test_df_summer = testing_data.to_frame(name='t2m')\n",
    "    test_df_summer['month_day'] = test_df_summer.index.strftime('%m-%d')\n",
    "    test_df_summer['above_25'] = test_df_summer['t2m'] > 298\n",
    "    days_over_25 = test_df_summer.groupby(test_df_summer.index.year)['above_25'].apply(sum)\n",
    "    column_to_append = days_over_25.tolist()\n",
    "    df_year.loc[:, f'days_above_25'] = column_to_append\n",
    "    \n",
    "    return df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_frost_days(df_weather_region, df_year):\n",
    "\n",
    "    testing_data = df_weather_region['t2m'].resample('D').max()\n",
    "    testing_data = testing_data.dropna()\n",
    "    test_df_frost = testing_data.to_frame(name='t2m')\n",
    "    test_df_frost['month_day'] = test_df_frost.index.strftime('%m-%d')\n",
    "    test_df_frost['under_0'] = test_df_frost['t2m'] < 273\n",
    "    days_under_0 = test_df_frost.groupby(test_df_frost.index.year)['under_0'].apply(sum)\n",
    "    column_to_append = days_under_0.tolist()\n",
    "    df_year.loc[:, f'days_under_0'] = column_to_append\n",
    "    \n",
    "    return df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the longest consecutive true streak\n",
    "\n",
    "def longest_consecutive_true_streak(series):\n",
    "    \n",
    "    # Convert the series to integers (True to 1, False to 0) for easier streak calculation\n",
    "    as_ints = series.astype(int)\n",
    "    # Calculate the difference to identify changes in streaks\n",
    "    diff = as_ints.diff()\n",
    "    # Start a new group every time there's a change from 0 to 1 (start of a new streak)\n",
    "    groups = (diff == 1).cumsum()\n",
    "    # Use the groups to isolate consecutive trues, then count them, keeping the max\n",
    "    streak_lengths = as_ints.groupby(groups).sum()\n",
    "    # Return the length of the longest streak\n",
    "    return streak_lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_dry_spell(df_weather_region, df_year):\n",
    "    \n",
    "    dist1_df_perci = df_weather_region.resample('D').sum()\n",
    "    dist1_df_perci_wo0 = dist1_df_perci[dist1_df_perci['tp'] != 0].copy()\n",
    "    \n",
    "    # Threshhold is 1mm or 0.001m, here precipitation is likely measured in m \n",
    "    \n",
    "    dist1_df_perci_wo0.loc[:,'less_than_0.001'] = dist1_df_perci_wo0['tp'] < 0.001\n",
    "    longest_dry_spell_per_year = dist1_df_perci_wo0.groupby(dist1_df_perci_wo0.index.year)['less_than_0.001'].apply(longest_consecutive_true_streak)\n",
    "    \n",
    "    column_to_append = longest_dry_spell_per_year.tolist()\n",
    "    df_year.loc[:, f'longest_dry_spell'] = column_to_append\n",
    "    \n",
    "    return df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_wet_spell(df_weather_region, df_year):\n",
    "    \n",
    "    dist1_df_perci = df_weather_region.resample('D').sum()\n",
    "    dist1_df_perci_wo0 = dist1_df_perci[dist1_df_perci['tp'] != 0].copy()\n",
    "    \n",
    "    # Threshhold is 1mm or 0.001m, here precipitation is likely measured in m \n",
    "    \n",
    "    dist1_df_perci_wo0.loc[:,'more_than_0.001'] = dist1_df_perci_wo0['tp'] > 0.001\n",
    "    longest_wet_spell_per_year = dist1_df_perci_wo0.groupby(dist1_df_perci_wo0.index.year)['more_than_0.001'].apply(longest_consecutive_true_streak)\n",
    "    \n",
    "    column_to_append = longest_wet_spell_per_year.tolist()\n",
    "    df_year.loc[:, f'longest_wet_spell'] = column_to_append\n",
    "    \n",
    "    return df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_95_precipitation(df_weather_region, df_weather_region_train, df_year):\n",
    "    \n",
    "    dist1_df_perci = df_weather_region.resample('D').sum()\n",
    "    dist1_df_perci_wo0 = dist1_df_perci[dist1_df_perci['tp'] != 0]\n",
    "    testing_data_pre = dist1_df_perci_wo0[\"tp\"]\n",
    "    \n",
    "    dist1_df_perci_train = df_weather_region_train.resample('D').sum()\n",
    "    dist1_df_perci_wo0_train = dist1_df_perci_train[dist1_df_perci_train['tp'] != 0]\n",
    "    training_data_pre = dist1_df_perci_wo0_train[\"tp\"]\n",
    "\n",
    "\n",
    "    # calculate for every day the 90% quantile\n",
    "    quantile_95_series = training_data_pre.groupby([training_data_pre.index.month, training_data_pre.index.day]).quantile(0.95)\n",
    "    quantile_95_series.index = quantile_95_series.index.map(lambda x: f\"{x[0]:02d}-{x[1]:02d}\")\n",
    "\n",
    "    test_df_pre = testing_data_pre.to_frame(name='tp')\n",
    "    test_df_pre['month_day'] = test_df_pre.index.strftime('%m-%d')\n",
    "\n",
    "    # map the 90th percentile values from quantile_90_series to the test series\n",
    "    test_df_pre['quantile_95'] = test_df_pre['month_day'].apply(lambda x: quantile_95_series.get(x, pd.NA))\n",
    "\n",
    "    # compare each test value to its corresponding 90th percentile value\n",
    "    test_df_pre['over_quantile_95'] = test_df_pre['tp'] > test_df_pre['quantile_95']\n",
    "    test_df_pre.drop(['month_day', 'quantile_95'], axis=1, inplace=True)\n",
    "\n",
    "    # Group the DataFrame by year, and apply the function to find the longest streak of True values\n",
    "    days_over_95 = test_df_pre.groupby(test_df_pre.index.year)['over_quantile_95'].apply(sum)\n",
    "\n",
    "    column_to_append = days_over_95.tolist()\n",
    "    df_year.loc[:, f'days_over_95_precipitation'] = column_to_append\n",
    "    \n",
    "    return df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_wave(df_weather_region, df_weather_region_train, df_year):\n",
    "\n",
    "    training_data = df_weather_region_train['t2m'].resample('D').max()\n",
    "    training_data = training_data.dropna()\n",
    "\n",
    "    testing_data = df_weather_region['t2m'].resample('D').max()\n",
    "    testing_data = testing_data.dropna()\n",
    "\n",
    "    # calculate for every day the 90% quantile\n",
    "    quantile_90_series = training_data.groupby([training_data.index.month, training_data.index.day]).quantile(0.9)\n",
    "    quantile_90_series.index = quantile_90_series.index.map(lambda x: f\"{x[0]:02d}-{x[1]:02d}\")\n",
    "\n",
    "    test_df = testing_data.to_frame(name='value')\n",
    "    test_df['month_day'] = test_df.index.strftime('%m-%d')\n",
    "\n",
    "    # map the 90th percentile values from quantile_90_series to the test series\n",
    "    test_df['quantile_90'] = test_df['month_day'].apply(lambda x: quantile_90_series.get(x, pd.NA))\n",
    "\n",
    "    # compare each test value to its corresponding 90th percentile value\n",
    "    test_df['is_above_quantile_90'] = test_df['value'] > test_df['quantile_90']\n",
    "    test_df.drop(['month_day', 'quantile_90'], axis=1, inplace=True)\n",
    "\n",
    "    # Group the DataFrame by year, and apply the function to find the longest streak of True values\n",
    "    longest_heat_streak_by_year = test_df.groupby(test_df.index.year)['is_above_quantile_90'].apply(longest_consecutive_true_streak)\n",
    "\n",
    "    column_to_append = longest_heat_streak_by_year.tolist()\n",
    "    df_year.loc[:, f'longest_heat_wave'] = column_to_append\n",
    "    \n",
    "    return df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_wave(df_weather_region, df_weather_region_train, df_year):\n",
    "    \n",
    "    training_data = df_weather_region_train['t2m'].resample('D').min()\n",
    "    training_data = training_data.dropna()\n",
    "\n",
    "    testing_data = df_weather_region['t2m'].resample('D').min()\n",
    "    testing_data = testing_data.dropna()\n",
    "    \n",
    "    # calculate for every day the 90% quantile\n",
    "    quantile_10_series = training_data.groupby([training_data.index.month, training_data.index.day]).quantile(0.1)\n",
    "    quantile_10_series.index = quantile_10_series.index.map(lambda x: f\"{x[0]:02d}-{x[1]:02d}\")\n",
    "\n",
    "    test_df_cold = testing_data.to_frame(name='value')\n",
    "    test_df_cold['month_day'] = test_df_cold.index.strftime('%m-%d')\n",
    "\n",
    "    # map the 90th percentile values from quantile_90_series to the test series\n",
    "    test_df_cold['quantile_10'] = test_df_cold['month_day'].apply(lambda x: quantile_10_series.get(x, pd.NA))\n",
    "\n",
    "    # compare each test value to its corresponding 90th percentile value\n",
    "    test_df_cold['is_under_quantile_10'] = test_df_cold['value'] < test_df_cold['quantile_10']\n",
    "    test_df_cold.drop(['month_day', 'quantile_10'], axis=1, inplace=True)\n",
    "\n",
    "    # Group the DataFrame by year, and apply the function to find the longest streak of True values\n",
    "    longest_streak_by_year_cold = test_df_cold.groupby(test_df_cold.index.year)['is_under_quantile_10'].apply(longest_consecutive_true_streak)\n",
    "\n",
    "    column_to_append = longest_streak_by_year_cold.tolist()\n",
    "    df_year.loc[:, f'longest_cold_wave'] = column_to_append\n",
    "    \n",
    "    return df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:21:45.031173Z",
     "iopub.status.busy": "2024-03-02T14:21:45.029922Z",
     "iopub.status.idle": "2024-03-02T14:21:45.055869Z",
     "shell.execute_reply": "2024-03-02T14:21:45.054159Z",
     "shell.execute_reply.started": "2024-03-02T14:21:45.031086Z"
    }
   },
   "outputs": [],
   "source": [
    "available_regions = [region for region in districts_with_full_data_list if region in df_rms['region_index'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_regions.remove(278)\n",
    "available_regions.remove(529)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:06:23.017781Z",
     "iopub.status.busy": "2024-03-02T15:06:23.017318Z",
     "iopub.status.idle": "2024-03-02T15:07:44.261074Z",
     "shell.execute_reply": "2024-03-02T15:07:44.259337Z",
     "shell.execute_reply.started": "2024-03-02T15:06:23.017749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "61\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "91\n",
      "92\n",
      "93\n",
      "95\n",
      "96\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "131\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "181\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "189\n",
      "190\n",
      "194\n",
      "211\n",
      "213\n",
      "214\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "241\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "271\n",
      "273\n",
      "276\n",
      "277\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "288\n",
      "304\n",
      "305\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "320\n",
      "331\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "394\n",
      "395\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "409\n",
      "410\n",
      "411\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "442\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "463\n",
      "464\n",
      "466\n",
      "467\n",
      "468\n",
      "471\n",
      "472\n",
      "486\n",
      "487\n",
      "488\n",
      "490\n",
      "491\n",
      "493\n",
      "494\n",
      "497\n",
      "499\n",
      "501\n",
      "502\n",
      "520\n",
      "588\n",
      "622\n"
     ]
    }
   ],
   "source": [
    "dfs_of_years = []\n",
    "for region in available_regions:\n",
    "    print(region)\n",
    "    df_weather_region, df_year, df_weather_region_train  = merge_canola_weather_data(region)\n",
    "    df_year.index = df_year.index.year\n",
    "    df_year['region'] = region\n",
    "    df_year = calc_temp_features(df_weather_region, df_year)\n",
    "    df_year = calc_spi_features(df_weather_region, df_year)\n",
    "    df_year = calc_summer_days(df_weather_region,df_year)\n",
    "    df_year = calc_frost_days(df_weather_region,df_year)\n",
    "    df_year = longest_dry_spell(df_weather_region,df_year)\n",
    "    df_year = longest_wet_spell(df_weather_region,df_year)\n",
    "    df_year = over_95_precipitation(df_weather_region, df_weather_region_train, df_year)\n",
    "    df_year = heat_wave(df_weather_region, df_weather_region_train, df_year)\n",
    "    df_year = cold_wave(df_weather_region, df_weather_region_train, df_year)\n",
    "    dfs_of_years.append(df_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:09:07.975451Z",
     "iopub.status.busy": "2024-03-02T15:09:07.974082Z",
     "iopub.status.idle": "2024-03-02T15:09:08.030078Z",
     "shell.execute_reply": "2024-03-02T15:09:08.028813Z",
     "shell.execute_reply.started": "2024-03-02T15:09:07.975400Z"
    }
   },
   "outputs": [],
   "source": [
    "df_full = pd.concat(dfs_of_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:09:09.507384Z",
     "iopub.status.busy": "2024-03-02T15:09:09.506268Z",
     "iopub.status.idle": "2024-03-02T15:09:09.549557Z",
     "shell.execute_reply": "2024-03-02T15:09:09.548476Z",
     "shell.execute_reply.started": "2024-03-02T15:09:09.507344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Canola</th>\n",
       "      <th>region</th>\n",
       "      <th>average_max_temp_in_4</th>\n",
       "      <th>average_max_temp_in_5</th>\n",
       "      <th>average_max_temp_in_6</th>\n",
       "      <th>average_max_temp_in_7</th>\n",
       "      <th>average_max_temp_in_8</th>\n",
       "      <th>average_max_temp_in_9</th>\n",
       "      <th>average_max_temp_in_10</th>\n",
       "      <th>SPI_in_4_time</th>\n",
       "      <th>...</th>\n",
       "      <th>SPI_in_10_time</th>\n",
       "      <th>SPI_in_10_tp</th>\n",
       "      <th>SPI_in_10_tp_calculated_index</th>\n",
       "      <th>days_above_25</th>\n",
       "      <th>days_under_0</th>\n",
       "      <th>longest_dry_spell</th>\n",
       "      <th>longest_wet_spell</th>\n",
       "      <th>days_over_95_precipitation</th>\n",
       "      <th>longest_heat_wave</th>\n",
       "      <th>longest_cold_wave</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0.127132</td>\n",
       "      <td>1</td>\n",
       "      <td>284.965759</td>\n",
       "      <td>290.526489</td>\n",
       "      <td>297.082458</td>\n",
       "      <td>299.412781</td>\n",
       "      <td>300.752075</td>\n",
       "      <td>296.791382</td>\n",
       "      <td>286.036377</td>\n",
       "      <td>1992-04-23 22:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-10-21 22:00:00</td>\n",
       "      <td>-1.862645e-09</td>\n",
       "      <td>-0.380289</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>2.520378</td>\n",
       "      <td>1</td>\n",
       "      <td>287.648346</td>\n",
       "      <td>291.984650</td>\n",
       "      <td>296.854401</td>\n",
       "      <td>297.652344</td>\n",
       "      <td>299.938751</td>\n",
       "      <td>292.543549</td>\n",
       "      <td>282.321960</td>\n",
       "      <td>1992-04-23 23:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-10-21 23:00:00</td>\n",
       "      <td>-1.862645e-09</td>\n",
       "      <td>-0.380289</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>-6.339489</td>\n",
       "      <td>1</td>\n",
       "      <td>283.242584</td>\n",
       "      <td>293.701874</td>\n",
       "      <td>295.445709</td>\n",
       "      <td>294.548126</td>\n",
       "      <td>297.033264</td>\n",
       "      <td>292.229004</td>\n",
       "      <td>285.503967</td>\n",
       "      <td>1992-04-24 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-10-22 00:00:00</td>\n",
       "      <td>-1.862645e-09</td>\n",
       "      <td>-0.380289</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>4.147971</td>\n",
       "      <td>1</td>\n",
       "      <td>284.697113</td>\n",
       "      <td>292.762695</td>\n",
       "      <td>293.425385</td>\n",
       "      <td>294.037415</td>\n",
       "      <td>296.177429</td>\n",
       "      <td>291.031281</td>\n",
       "      <td>284.688232</td>\n",
       "      <td>1992-04-24 01:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-10-22 01:00:00</td>\n",
       "      <td>-1.862645e-09</td>\n",
       "      <td>-0.380289</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>2.081733</td>\n",
       "      <td>1</td>\n",
       "      <td>285.144440</td>\n",
       "      <td>293.432678</td>\n",
       "      <td>294.774536</td>\n",
       "      <td>297.061951</td>\n",
       "      <td>297.072021</td>\n",
       "      <td>294.903839</td>\n",
       "      <td>286.052094</td>\n",
       "      <td>1992-04-24 02:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-10-22 02:00:00</td>\n",
       "      <td>-1.862645e-09</td>\n",
       "      <td>-0.380289</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>-0.642088</td>\n",
       "      <td>622</td>\n",
       "      <td>277.338013</td>\n",
       "      <td>294.617126</td>\n",
       "      <td>296.039825</td>\n",
       "      <td>297.508179</td>\n",
       "      <td>297.946716</td>\n",
       "      <td>285.104767</td>\n",
       "      <td>280.153503</td>\n",
       "      <td>1992-04-25 02:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-10-23 02:00:00</td>\n",
       "      <td>-1.862645e-09</td>\n",
       "      <td>-0.389745</td>\n",
       "      <td>51</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>5.176214</td>\n",
       "      <td>622</td>\n",
       "      <td>283.511230</td>\n",
       "      <td>289.193604</td>\n",
       "      <td>293.939178</td>\n",
       "      <td>295.673492</td>\n",
       "      <td>293.541107</td>\n",
       "      <td>289.933136</td>\n",
       "      <td>280.083069</td>\n",
       "      <td>1992-04-25 03:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-10-23 03:00:00</td>\n",
       "      <td>-1.862645e-09</td>\n",
       "      <td>-0.389745</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>-1.002221</td>\n",
       "      <td>622</td>\n",
       "      <td>277.344818</td>\n",
       "      <td>290.024719</td>\n",
       "      <td>293.347931</td>\n",
       "      <td>296.085846</td>\n",
       "      <td>296.669983</td>\n",
       "      <td>291.383881</td>\n",
       "      <td>279.867188</td>\n",
       "      <td>1992-04-25 04:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-10-23 04:00:00</td>\n",
       "      <td>-1.862645e-09</td>\n",
       "      <td>-0.389745</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>-18.980060</td>\n",
       "      <td>622</td>\n",
       "      <td>284.660583</td>\n",
       "      <td>290.581299</td>\n",
       "      <td>297.739471</td>\n",
       "      <td>299.774323</td>\n",
       "      <td>298.133942</td>\n",
       "      <td>293.659637</td>\n",
       "      <td>284.803772</td>\n",
       "      <td>1992-04-25 05:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-10-23 05:00:00</td>\n",
       "      <td>-1.862645e-09</td>\n",
       "      <td>-0.389745</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>0.548040</td>\n",
       "      <td>622</td>\n",
       "      <td>279.618134</td>\n",
       "      <td>289.977875</td>\n",
       "      <td>294.012695</td>\n",
       "      <td>296.951447</td>\n",
       "      <td>298.845917</td>\n",
       "      <td>295.413269</td>\n",
       "      <td>287.941833</td>\n",
       "      <td>1992-04-25 06:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-10-23 06:00:00</td>\n",
       "      <td>-1.862645e-09</td>\n",
       "      <td>-0.389745</td>\n",
       "      <td>41</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5973 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Canola  region  average_max_temp_in_4  average_max_temp_in_5  \\\n",
       "Year                                                                    \n",
       "1990   0.127132       1             284.965759             290.526489   \n",
       "1991   2.520378       1             287.648346             291.984650   \n",
       "1992  -6.339489       1             283.242584             293.701874   \n",
       "1993   4.147971       1             284.697113             292.762695   \n",
       "1994   2.081733       1             285.144440             293.432678   \n",
       "...         ...     ...                    ...                    ...   \n",
       "2018  -0.642088     622             277.338013             294.617126   \n",
       "2019   5.176214     622             283.511230             289.193604   \n",
       "2020  -1.002221     622             277.344818             290.024719   \n",
       "2021 -18.980060     622             284.660583             290.581299   \n",
       "2022   0.548040     622             279.618134             289.977875   \n",
       "\n",
       "      average_max_temp_in_6  average_max_temp_in_7  average_max_temp_in_8  \\\n",
       "Year                                                                        \n",
       "1990             297.082458             299.412781             300.752075   \n",
       "1991             296.854401             297.652344             299.938751   \n",
       "1992             295.445709             294.548126             297.033264   \n",
       "1993             293.425385             294.037415             296.177429   \n",
       "1994             294.774536             297.061951             297.072021   \n",
       "...                     ...                    ...                    ...   \n",
       "2018             296.039825             297.508179             297.946716   \n",
       "2019             293.939178             295.673492             293.541107   \n",
       "2020             293.347931             296.085846             296.669983   \n",
       "2021             297.739471             299.774323             298.133942   \n",
       "2022             294.012695             296.951447             298.845917   \n",
       "\n",
       "      average_max_temp_in_9  average_max_temp_in_10       SPI_in_4_time  ...  \\\n",
       "Year                                                                     ...   \n",
       "1990             296.791382              286.036377 1992-04-23 22:00:00  ...   \n",
       "1991             292.543549              282.321960 1992-04-23 23:00:00  ...   \n",
       "1992             292.229004              285.503967 1992-04-24 00:00:00  ...   \n",
       "1993             291.031281              284.688232 1992-04-24 01:00:00  ...   \n",
       "1994             294.903839              286.052094 1992-04-24 02:00:00  ...   \n",
       "...                     ...                     ...                 ...  ...   \n",
       "2018             285.104767              280.153503 1992-04-25 02:00:00  ...   \n",
       "2019             289.933136              280.083069 1992-04-25 03:00:00  ...   \n",
       "2020             291.383881              279.867188 1992-04-25 04:00:00  ...   \n",
       "2021             293.659637              284.803772 1992-04-25 05:00:00  ...   \n",
       "2022             295.413269              287.941833 1992-04-25 06:00:00  ...   \n",
       "\n",
       "          SPI_in_10_time  SPI_in_10_tp SPI_in_10_tp_calculated_index  \\\n",
       "Year                                                                   \n",
       "1990 1992-10-21 22:00:00 -1.862645e-09                     -0.380289   \n",
       "1991 1992-10-21 23:00:00 -1.862645e-09                     -0.380289   \n",
       "1992 1992-10-22 00:00:00 -1.862645e-09                     -0.380289   \n",
       "1993 1992-10-22 01:00:00 -1.862645e-09                     -0.380289   \n",
       "1994 1992-10-22 02:00:00 -1.862645e-09                     -0.380289   \n",
       "...                  ...           ...                           ...   \n",
       "2018 1992-10-23 02:00:00 -1.862645e-09                     -0.389745   \n",
       "2019 1992-10-23 03:00:00 -1.862645e-09                     -0.389745   \n",
       "2020 1992-10-23 04:00:00 -1.862645e-09                     -0.389745   \n",
       "2021 1992-10-23 05:00:00 -1.862645e-09                     -0.389745   \n",
       "2022 1992-10-23 06:00:00 -1.862645e-09                     -0.389745   \n",
       "\n",
       "      days_above_25  days_under_0 longest_dry_spell  longest_wet_spell  \\\n",
       "Year                                                                     \n",
       "1990             72             0                 7                 11   \n",
       "1991             51             4                 6                 13   \n",
       "1992             37             2                 7                 11   \n",
       "1993             26             1                 6                 14   \n",
       "1994             38             0                 9                 15   \n",
       "...             ...           ...               ...                ...   \n",
       "2018             51            11                 9                 20   \n",
       "2019             11             3                 7                 33   \n",
       "2020             23            17                 6                 10   \n",
       "2021             55             0                17                 10   \n",
       "2022             41             5                18                 22   \n",
       "\n",
       "      days_over_95_precipitation longest_heat_wave  longest_cold_wave  \n",
       "Year                                                                   \n",
       "1990                          11                 7                  5  \n",
       "1991                          18                 6                  4  \n",
       "1992                           8                 5                  7  \n",
       "1993                          10                 5                  5  \n",
       "1994                          16                 3                  6  \n",
       "...                          ...               ...                ...  \n",
       "2018                          13                 4                  9  \n",
       "2019                          13                 3                  4  \n",
       "2020                           9                 3                 11  \n",
       "2021                           7                 8                  3  \n",
       "2022                           9                 7                  4  \n",
       "\n",
       "[5973 rows x 37 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4146483,
     "sourceId": 7175566,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4158112,
     "sourceId": 7690514,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4526286,
     "sourceId": 7743492,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
